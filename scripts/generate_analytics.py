#!/usr/bin/env python3
"""
Generate sermon analytics data for Jekyll site.

This script fetches sermon data from the API, analyzes all sermons,
and generates JSON files with analytics data that can be used by the Jekyll site.
"""

import os
import json
import time
import re
import httpx
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict, Counter

# Configuration
API_URL = os.environ.get("API_URL", "https://sermon-search-api-8fok.onrender.com")
OUTPUT_DIR = Path("_data/analytics")
REQUEST_TIMEOUT = 30  # seconds
MAX_RETRIES = 3
RETRY_DELAY = 5  # seconds
BATCH_SIZE = 10  # Number of sermons to process in parallel

# Bible book data
OLD_TESTAMENT = [
    'Genesis', 'Exodus', 'Leviticus', 'Numbers', 'Deuteronomy', 'Joshua', 'Judges', 'Ruth', 
    '1 Samuel', '2 Samuel', '1 Kings', '2 Kings', '1 Chronicles', '2 Chronicles', 'Ezra', 
    'Nehemiah', 'Esther', 'Job', 'Psalms', 'Proverbs', 'Ecclesiastes', 'Song of Solomon', 
    'Isaiah', 'Jeremiah', 'Lamentations', 'Ezekiel', 'Daniel', 'Hosea', 'Joel', 'Amos', 
    'Obadiah', 'Jonah', 'Micah', 'Nahum', 'Habakkuk', 'Zephaniah', 'Haggai', 'Zechariah', 'Malachi'
]

NEW_TESTAMENT = [
    'Matthew', 'Mark', 'Luke', 'John', 'Acts',
    'Romans', '1 Corinthians', '2 Corinthians', 'Galatians', 'Ephesians', 'Philippians', 
    'Colossians', '1 Thessalonians', '2 Thessalonians', '1 Timothy', '2 Timothy', 'Titus', 
    'Philemon', 'Hebrews', 'James', '1 Peter', '2 Peter', '1 John', '2 John', '3 John', 'Jude',
    'Revelation'
]

BIBLE_BOOKS = OLD_TESTAMENT + NEW_TESTAMENT

# Regular expression for Bible references
BIBLE_REF_PATTERN = r'\b(Genesis|Exodus|Leviticus|Numbers|Deuteronomy|Joshua|Judges|Ruth|1 Samuel|2 Samuel|1 Kings|2 Kings|1 Chronicles|2 Chronicles|Ezra|Nehemiah|Esther|Job|Psalms|Psalm|Proverbs|Ecclesiastes|Song of Solomon|Isaiah|Jeremiah|Lamentations|Ezekiel|Daniel|Hosea|Joel|Amos|Obadiah|Jonah|Micah|Nahum|Habakkuk|Zephaniah|Haggai|Zechariah|Malachi|Matthew|Mark|Luke|John|Acts|Romans|1 Corinthians|2 Corinthians|Galatians|Ephesians|Philippians|Colossians|1 Thessalonians|2 Thessalonians|1 Timothy|2 Timothy|Titus|Philemon|Hebrews|James|1 Peter|2 Peter|1 John|2 John|3 John|Jude|Revelation)\s+(\d+)(?::(\d+)(?:-(\d+))?)?'

def setup_directory():
    """Create the output directory if it doesn't exist."""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # Create references subdirectory
    references_dir = OUTPUT_DIR / "references"
    references_dir.mkdir(exist_ok=True)

async def fetch_with_retry(client, url, method="GET", data=None):
    """Fetch data from the API with retry logic."""
    for attempt in range(MAX_RETRIES):
        try:
            if method == "GET":
                response = await client.get(url, timeout=REQUEST_TIMEOUT)
            else:
                response = await client.post(url, json=data, timeout=REQUEST_TIMEOUT)
            
            response.raise_for_status()
            return response.json()
        except httpx.HTTPStatusError as e:
            print(f"HTTP error occurred: {e} - Attempt {attempt + 1} of {MAX_RETRIES}")
        except httpx.RequestError as e:
            print(f"Request error occurred: {e} - Attempt {attempt + 1} of {MAX_RETRIES}")
        
        if attempt < MAX_RETRIES - 1:
            print(f"Retrying in {RETRY_DELAY} seconds...")
            time.sleep(RETRY_DELAY)
    
    raise Exception(f"Failed to fetch {url} after {MAX_RETRIES} attempts")

async def load_sermon_metadata():
    """
    Load sermon metadata from the pre-processed file.
    This data should have been generated by process_existing_metadata.py
    """
    sermons_file = OUTPUT_DIR / "sermons.json"
    
    if not sermons_file.exists():
        print(f"Warning: Sermons metadata file not found at {sermons_file}")
        return {}
    
    try:
        with open(sermons_file, 'r') as f:
            sermon_details = json.load(f)
        
        print(f"Loaded metadata for {len(sermon_details)} sermons")
        return sermon_details
    except Exception as e:
        print(f"Error loading sermon metadata: {e}")
        return {}

async def fetch_sermon_chunks(client, video_id):
    """Fetch chunks for a single sermon."""
    try:
        print(f"Fetching chunks for sermon {video_id}")
        data = await fetch_with_retry(client, f"{API_URL}/sermons/{video_id}")
        return data.get("chunks", [])
    except Exception as e:
        print(f"Error fetching chunks for sermon {video_id}: {e}")
        return []

async def fetch_all_sermon_chunks(client, sermon_details):
    """
    Fetch chunks for all sermons using batched requests to avoid overwhelming the API.
    """
    all_chunks = []
    
    # Get list of video IDs
    video_ids = list(sermon_details.keys())
    
    # Process sermons in batches
    for i in range(0, len(video_ids), BATCH_SIZE):
        batch = video_ids[i:i + BATCH_SIZE]
        print(f"Processing batch {i//BATCH_SIZE + 1} of {(len(video_ids) + BATCH_SIZE - 1)//BATCH_SIZE}")
        
        # Create a list of tasks to fetch chunks for each sermon in the batch
        tasks = [fetch_sermon_chunks(client, video_id) for video_id in batch]
        
        # Process each sermon's chunks
        for j, video_id in enumerate(batch):
            try:
                chunks = await tasks[j]
                if chunks:
                    # Add video_id to each chunk if not already present
                    for chunk in chunks:
                        if "video_id" not in chunk:
                            chunk["video_id"] = video_id
                    
                    # Store chunks
                    all_chunks.extend(chunks)
                    
                    # Update sermon details with chunk count
                    if video_id in sermon_details:
                        sermon_details[video_id]["chunk_count"] = len(chunks)
                    
                    print(f"Sermon {video_id} has {len(chunks)} chunks")
                else:
                    print(f"No chunks found for sermon {video_id}")
            except Exception as e:
                print(f"Error processing sermon {video_id}: {e}")
        
        # Add a small delay to avoid API rate limits
        time.sleep(1)
    
    return all_chunks

def analyze_bible_references(chunks):
    """
    Extract and analyze Bible references from sermon chunks.
    """
    print("Analyzing Bible references...")
    
    # Initialize counters
    book_counts = Counter()
    chapter_counts = Counter()
    verse_counts = Counter()
    testament_counts = {"Old Testament": 0, "New Testament": 0}
    
    # Analyze each chunk
    total_references = 0
    for chunk in chunks:
        text = chunk.get("text", "")
        if not text:
            continue
            
        # Find all Bible references in the text
        matches = re.finditer(BIBLE_REF_PATTERN, text, re.IGNORECASE)
        
        for match in matches:
            total_references += 1
            
            # Standardize book name
            book = match.group(1).title()
            if book == "Psalm":
                book = "Psalms"
            
            # Count the book reference
            book_counts[book] += 1
            
            # Count testament
            if book in OLD_TESTAMENT:
                testament_counts["Old Testament"] += 1
            elif book in NEW_TESTAMENT:
                testament_counts["New Testament"] += 1
            
            # If chapter is specified
            if match.group(2):
                chapter = match.group(2)
                chapter_key = f"{book} {chapter}"
                chapter_counts[chapter_key] += 1
                
                # If verse is specified
                if match.group(3):
                    verse = match.group(3)
                    verse_key = f"{book} {chapter}:{verse}"
                    verse_counts[verse_key] += 1
    
    return {
        "total_references": total_references,
        "book_counts": dict(book_counts),
        "chapter_counts": dict(chapter_counts),
        "verse_counts": dict(verse_counts),
        "testament_counts": testament_counts
    }

def group_sermons_by_time(sermon_details):
    """
    Group sermons by year and month for time-based filtering.
    """
    print("Grouping sermons by time periods...")
    
    by_year = defaultdict(list)
    by_month = defaultdict(list)
    by_year_month = defaultdict(list)
    
    for video_id, sermon in sermon_details.items():
        publish_date = sermon.get("publish_date", "")
        
        try:
            if publish_date:
                date = datetime.fromisoformat(publish_date.replace('Z', '+00:00'))
                
                # Group by year
                year = date.year
                by_year[year].append(video_id)
                
                # Group by month (regardless of year)
                month = date.month
                by_month[month].append(video_id)
                
                # Group by year-month
                year_month = f"{year}-{month:02d}"
                by_year_month[year_month].append(video_id)
        except (ValueError, TypeError) as e:
            print(f"Error parsing date for sermon {video_id}: {e}")
    
    return {
        "by_year": dict(by_year),
        "by_month": dict(by_month),
        "by_year_month": dict(by_year_month)
    }

def generate_reference_occurrences(chunks, sermon_details):
    """
    Generate occurrence data for individual Bible references.
    This allows for showing exactly where each reference appears in sermons.
    """
    print("Building reference occurrence data...")
    
    # Create a references directory
    reference_dir = OUTPUT_DIR / "references"
    reference_dir.mkdir(exist_ok=True)
    
    # Dictionary to track references and their occurrences
    reference_occurrences = {}
    
    # Process chunks to find references
    for chunk in chunks:
        text = chunk.get("text", "")
        if not text:
            continue
            
        # Get sermon details
        video_id = chunk.get("video_id")
        if not video_id or video_id not in sermon_details:
            continue
            
        sermon = sermon_details[video_id]
        start_time = chunk.get("start_time", 0)
        
        # Find all Bible references
        matches = re.finditer(BIBLE_REF_PATTERN, text, re.IGNORECASE)
        
        for match in matches:
            # Create reference key (e.g., "John 3:16")
            book = match.group(1).title()
            if book == "Psalm":
                book = "Psalms"
                
            reference = book
            
            if match.group(2):  # Chapter
                reference += f" {match.group(2)}"
                
                if match.group(3):  # Verse
                    reference += f":{match.group(3)}"
            
            # Initialize if this is a new reference
            if reference not in reference_occurrences:
                reference_occurrences[reference] = []
            
            # Add occurrence data
            reference_occurrences[reference].append({
                "sermon_id": video_id,
                "sermon_title": sermon.get("title", "Unknown Sermon"),
                "timestamp": start_time,
                "url": f"https://www.youtube.com/watch?v={video_id}&t={int(start_time)}",
                "text": text,
                "channel": sermon.get("channel", "Unknown Channel"),
                "publishedAt": sermon.get("publish_date", "")
            })
    
    # Generate files for references with the most occurrences first
    reference_counts = {ref: len(occs) for ref, occs in reference_occurrences.items()}
    sorted_references = sorted(reference_counts.items(), key=lambda x: x[1], reverse=True)
    
    # Only generate files for references with multiple occurrences to save space
    # Limit to top 500 references to avoid generating too many files
    print(f"Generating files for top 500 references with multiple occurrences...")
    count = 0
    for reference, _ in sorted_references:
        occurrences = reference_occurrences[reference]
        if len(occurrences) < 2:
            continue
            
        # Create a safe filename
        safe_reference = reference.replace(":", "_").replace(" ", "_")
        file_path = reference_dir / f"{safe_reference}.json"
        
        # Write the occurrence data
        with open(file_path, "w") as f:
            json.dump({"reference": reference, "occurrences": occurrences}, f, indent=2)
        
        count += 1
        if count >= 500:
            break
            
    print(f"Generated occurrence data for {count} references")
    
    # Return mapping of references to occurrence counts for the index file
    return {reference: len(occurrences) for reference, occurrences in reference_occurrences.items()}

def generate_analytics_files(sermon_details, chunks, reference_data, time_grouping, reference_counts):
    """
    Generate all the JSON files needed for the analytics dashboard.
    """
    print("Generating analytics JSON files...")
    
    # 1. Generate summary file with overall statistics
    summary = {
        "total_sermons": len(sermon_details),
        "total_chunks": len(chunks),
        "total_references": reference_data["total_references"],
        "top_books": dict(Counter(reference_data["book_counts"]).most_common(10)),
        "testament_distribution": reference_data["testament_counts"],
        "generated_at": datetime.now().isoformat()
    }
    
    with open(OUTPUT_DIR / "summary.json", "w") as f:
        json.dump(summary, f, indent=2)
    
    # 2. Generate book references file
    with open(OUTPUT_DIR / "books.json", "w") as f:
        json.dump(reference_data["book_counts"], f, indent=2)
    
    # 3. Generate chapter references file (top 100)
    top_chapters = dict(Counter(reference_data["chapter_counts"]).most_common(100))
    with open(OUTPUT_DIR / "chapters.json", "w") as f:
        json.dump(top_chapters, f, indent=2)
    
    # 4. Generate verse references file (top 100)
    top_verses = dict(Counter(reference_data["verse_counts"]).most_common(100))
    with open(OUTPUT_DIR / "verses.json", "w") as f:
        json.dump(top_verses, f, indent=2)
    
    # 5. Save updated sermon details file
    # Note: We're not overwriting the sermon details file here since it was already
    # generated by process_existing_metadata.py and we just added chunk counts
    
    # 6. Generate time grouping files
    with open(OUTPUT_DIR / "time_grouping.json", "w") as f:
        json.dump(time_grouping, f, indent=2)
    
    # 7. Generate references index file
    with open(OUTPUT_DIR / "references_index.json", "w") as f:
        json.dump(reference_counts, f, indent=2)
    
    print("Analytics files generated successfully.")

async def main():
    """Main function to generate all analytics data."""
    print(f"Starting sermon analytics generation at {datetime.now().isoformat()}")
    
    # Create output directory
    setup_directory()
    
    # Load sermon metadata (should be pre-processed by process_existing_metadata.py)
    sermon_details = await load_sermon_metadata()
    
    if not sermon_details:
        print("No sermon metadata found. Exiting.")
        return
    
    async with httpx.AsyncClient() as client:
        # Fetch chunks for all sermons
        all_chunks = await fetch_all_sermon_chunks(client, sermon_details)
        
        if not all_chunks:
            print("No chunks found. Exiting.")
            return
            
        print(f"Fetched {len(all_chunks)} chunks from {len(sermon_details)} sermons")
        
        # Analyze Bible references
        reference_data = analyze_bible_references(all_chunks)
        
        # Group sermons by time periods
        time_grouping = group_sermons_by_time(sermon_details)
        
        # Generate reference occurrences
        reference_counts = generate_reference_occurrences(all_chunks, sermon_details)
        
        # Generate analytics files
        generate_analytics_files(sermon_details, all_chunks, reference_data, time_grouping, reference_counts)
    
    print(f"Sermon analytics generation completed at {datetime.now().isoformat()}")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())